{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "195a1b9f-5fea-4a78-9461-3442983ef28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout,Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import joblib\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GRU, Dense, Dropout, Conv1D\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from typing import Dict, Tuple\n",
    "import pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8a4280d-c086-40ce-84ef-98eece07cd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDB connection successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pymongo import MongoClient\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "mongo_url = os.getenv(\"MONGO_URL\")\n",
    "if not mongo_url:\n",
    "    raise ValueError(\"MONGO_URL not set in .env\")\n",
    "\n",
    "client = MongoClient(mongo_url, serverSelectionTimeoutMS=5000)  # 5-second timeout\n",
    "db = client[\"storage_simulation\"]\n",
    "collection = db[\"usage_logs\"]\n",
    "\n",
    "# Optional: test the connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"MongoDB connection successful\")\n",
    "except Exception as e:\n",
    "    print(f\"MongoDB connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caf8bda7-b5d9-4cca-b9f4-9a2b5892588a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# here we we use 4 hour aggrigation so 7 per day\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "HORIZONS = {   \n",
    "    '1_month': 180,   # 30 days (6*30=180)  \n",
    "}\n",
    "SEQ_LENGTH = 42  # 7 days of historical data\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2cf0c9c-a096-4041-bdbd-0fb43bdc6f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data() -> Dict[str, dict]:\n",
    "    \"\"\"Load and preprocess data with proper feature engineering\"\"\"\n",
    "    raw_data = pd.DataFrame(list(collection.find()))\n",
    "    raw_data = raw_data.drop(columns=['_id'])\n",
    "    raw_data['timestamp'] = pd.to_datetime(raw_data['timestamp'])\n",
    "\n",
    "    print(\"\\n🔍 Data Diagnostics:\")\n",
    "    print(f\"Total records: {len(raw_data)}\")\n",
    "    print(\"Unique directories:\", raw_data['directory'].unique())\n",
    "\n",
    "    processed = {}\n",
    "    for directory in raw_data['directory'].unique():\n",
    "        df = raw_data[raw_data['directory'] == directory].copy()\n",
    "        df = df.sort_values('timestamp').set_index('timestamp')\n",
    "\n",
    "        # Resample to 4-hour intervals\n",
    "        # The 'directory' column is excluded from the mean calculation\n",
    "        df = df[['storage_gb']].resample('4h').mean().ffill()\n",
    "\n",
    "        # Feature engineering\n",
    "        df['hour'] = df.index.hour\n",
    "        df['time_sin'] = np.sin(2 * np.pi * df.index.hour/23)\n",
    "        df['time_cos'] = np.cos(2 * np.pi * df.index.hour/23)\n",
    "\n",
    "        # Scale storage_gb\n",
    "        scaler = MinMaxScaler()\n",
    "        df['scaled_gb'] = scaler.fit_transform(df[['storage_gb']])\n",
    "\n",
    "        processed[directory] = {\n",
    "            'data': df[['scaled_gb', 'time_sin', 'time_cos']],\n",
    "            'original': df['storage_gb'],\n",
    "            'scaler': scaler\n",
    "        }\n",
    "\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a9a0ce5-0d6b-40e0-951f-699c243cdf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(features: np.ndarray, targets: np.ndarray,\n",
    "                    seq_length: int, horizon: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Create sequences with validation\"\"\"\n",
    "    X, y = [], []\n",
    "    max_start = len(features) - seq_length - horizon\n",
    "    if max_start < 0:\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    for i in range(max_start + 1):\n",
    "        X.append(features[i:i+seq_length])\n",
    "        y.append(targets[i+seq_length:i+seq_length+horizon])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "921a7888-19e2-4adc-bfce-6f7850e7cb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "def build_model(input_shape: Tuple[int, int], output_steps: int) -> Model:\n",
    "    \"\"\"Optimized forecasting model architecture\"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Temporal pattern extraction\n",
    "    x = Conv1D(64, 3, activation='relu', padding='causal')(inputs)\n",
    "    x = GRU(128, return_sequences=True)(x)\n",
    "    x = GRU(64)(x)\n",
    "\n",
    "    # Prediction head\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(output_steps)(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "782e6996-c941-4c16-a311-77fea57519f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "\n",
    "def save_model_and_scaler(model, scaler, name):\n",
    "    notebooks_dir = os.getcwd()\n",
    "\n",
    "    # Ensure name is only a base name, not a path\n",
    "    safe_name = os.path.basename(name)\n",
    "    safe_name = safe_name.replace('/', '_').replace('\\\\', '_')\n",
    "\n",
    "    # Create models directory\n",
    "    models_dir = os.path.join(notebooks_dir, 'models')\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "    # Final model path\n",
    "    model_path = os.path.join(models_dir, f\"{safe_name}_monthly_forecast_model.keras\")\n",
    "    model.save(model_path)\n",
    "    print(f\"Model saved at: {model_path}\")\n",
    "\n",
    "    # Create scalers directory\n",
    "    scalers_dir = os.path.join(notebooks_dir, 'scalers')\n",
    "    os.makedirs(scalers_dir, exist_ok=True)\n",
    "\n",
    "    # Final scaler path\n",
    "    scaler_path = os.path.join(scalers_dir, f\"{safe_name}_monthly_scaler.pkl\")\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    print(f\"Scaler saved at: {scaler_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a34955dc-27c6-4c17-875d-85efe8beab07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(data_dict: Dict) -> Tuple[Dict, Dict, Dict]:\n",
    "    \"\"\"Enhanced training with proper validation\"\"\"\n",
    "    modeles = {}\n",
    "    metrics = {}\n",
    "\n",
    "    for name, data in data_dict.items():\n",
    "        print(f\"\\n⚡ Processing {name}\")\n",
    "        df = data['data']\n",
    "        scaler = data['scaler']\n",
    "\n",
    "        # Prepare data\n",
    "        total_points = len(df)\n",
    "        test_size = HORIZONS['1_month'] + SEQ_LENGTH\n",
    "        split_idx = total_points - test_size\n",
    "\n",
    "        if split_idx < SEQ_LENGTH:\n",
    "            print(f\"⚠️ Insufficient data for {name}\")\n",
    "            continue\n",
    "\n",
    "        # Create sequences\n",
    "        X_train, y_train = create_sequences(\n",
    "            df.values[:split_idx],\n",
    "            df['scaled_gb'].values[:split_idx],\n",
    "            SEQ_LENGTH, HORIZONS['1_month']\n",
    "        )\n",
    "        X_test, y_test = create_sequences(\n",
    "            df.values[split_idx:],\n",
    "            df['scaled_gb'].values[split_idx:],\n",
    "            SEQ_LENGTH, HORIZONS['1_month']\n",
    "        )\n",
    "\n",
    "        if len(X_train) == 0 or len(X_test) == 0:\n",
    "            print(f\"🚫 Sequence creation failed for {name}\")\n",
    "            continue\n",
    "\n",
    "        # Model setup\n",
    "        model = build_model((SEQ_LENGTH, 3), HORIZONS['1_month'])\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_test, y_test),\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            callbacks=[\n",
    "                EarlyStopping(patience=7, restore_best_weights=True),\n",
    "                ModelCheckpoint(f'best_{name}.keras', save_best_only=True)\n",
    "            ],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Generate predictions\n",
    "        test_pred = model.predict(X_test)\n",
    "        metrics[name] = {}\n",
    "\n",
    "        # Calculate metrics for each horizon\n",
    "        for horizon_name, steps in HORIZONS.items():\n",
    "            preds = test_pred[:, :steps].reshape(-1, 1)\n",
    "            true = y_test[:, :steps].reshape(-1, 1)\n",
    "\n",
    "            # Inverse transform predictions\n",
    "            preds_gb = scaler.inverse_transform(preds).reshape(-1, steps)\n",
    "            true_gb = scaler.inverse_transform(true).reshape(-1, steps)\n",
    "\n",
    "            # Calculate RMSE\n",
    "            rmse = np.sqrt(mean_squared_error(true_gb, preds_gb))\n",
    "\n",
    "            metrics[name][horizon_name] = {\n",
    "                'rmse': rmse,\n",
    "                'predictions': preds_gb[0],\n",
    "                'true': true_gb[0]\n",
    "            }\n",
    "\n",
    "        modeles[name] = model\n",
    "        save_model_and_scaler(model, scaler, name)\n",
    "\n",
    "    return modeles, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95108374-a2ec-4ae3-8ea9-63c3e77f9c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(name: str, data: dict, metrics: dict):\n",
    "    \"\"\"Enhanced plotting with actual dates\"\"\"\n",
    "    if name not in metrics:\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    original = data['original']\n",
    "\n",
    "    # Get test period dates\n",
    "    test_dates = original.index[-HORIZONS['1_month']:]\n",
    "\n",
    "    for horizon in HORIZONS:\n",
    "        if horizon not in metrics[name]:\n",
    "            continue\n",
    "\n",
    "        steps = HORIZONS[horizon]\n",
    "        preds = metrics[name][horizon]['predictions'][:steps]\n",
    "        true = metrics[name][horizon]['true'][:steps]\n",
    "        dates = test_dates[:steps]\n",
    "\n",
    "        plt.plot(dates, preds, label=f'{horizon} forecast')\n",
    "        plt.fill_between(dates,\n",
    "                        preds * 0.95,\n",
    "                        preds * 1.05,\n",
    "                        alpha=0.1)\n",
    "        plt.plot(dates, true, '--', label='Actual')\n",
    "\n",
    "    plt.title(f'{name} Storage Forecast')\n",
    "    plt.ylabel('Storage (GB)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d65567d-bbab-4d48-b036-04b63202bb32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Data Diagnostics:\n",
      "Total records: 292504\n",
      "Unique directories: ['/scratch' '/projects' '/customer' '/info']\n",
      "\n",
      "⚡ Processing /scratch\n",
      "Epoch 1/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 317ms/step - loss: 0.2245 - val_loss: 0.0724\n",
      "Epoch 2/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 367ms/step - loss: 0.0681 - val_loss: 0.0090\n",
      "Epoch 3/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 336ms/step - loss: 0.0298 - val_loss: 0.0033\n",
      "Epoch 4/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 298ms/step - loss: 0.0246 - val_loss: 0.0013\n",
      "Epoch 5/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 326ms/step - loss: 0.0224 - val_loss: 0.0013\n",
      "Epoch 6/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 376ms/step - loss: 0.0208 - val_loss: 0.0012\n",
      "Epoch 7/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 360ms/step - loss: 0.0187 - val_loss: 0.0042\n",
      "Epoch 8/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 230ms/step - loss: 0.0186 - val_loss: 0.0011\n",
      "Epoch 9/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 288ms/step - loss: 0.0176 - val_loss: 0.0024\n",
      "Epoch 10/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 301ms/step - loss: 0.0173 - val_loss: 0.0027\n",
      "Epoch 11/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 381ms/step - loss: 0.0162 - val_loss: 0.0022\n",
      "Epoch 12/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 315ms/step - loss: 0.0166 - val_loss: 0.0018\n",
      "Epoch 13/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 311ms/step - loss: 0.0161 - val_loss: 0.0044\n",
      "Epoch 14/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 311ms/step - loss: 0.0164 - val_loss: 0.0011\n",
      "Epoch 15/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 417ms/step - loss: 0.0162 - val_loss: 0.0016\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 562ms/step\n",
      "Model saved at: D:\\Projects\\HPE-StoragePrediction\\notebooks\\models\\scratch_monthly_forecast_model.keras\n",
      "Scaler saved at: D:\\Projects\\HPE-StoragePrediction\\notebooks\\scalers\\scratch_monthly_scaler.pkl\n",
      "\n",
      "⚡ Processing /projects\n",
      "Epoch 1/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 327ms/step - loss: 0.2656 - val_loss: 0.2357\n",
      "Epoch 2/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 240ms/step - loss: 0.0771 - val_loss: 0.0258\n",
      "Epoch 3/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 433ms/step - loss: 0.0278 - val_loss: 0.0215\n",
      "Epoch 4/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 429ms/step - loss: 0.0195 - val_loss: 0.0081\n",
      "Epoch 5/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 388ms/step - loss: 0.0160 - val_loss: 0.0071\n",
      "Epoch 6/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 416ms/step - loss: 0.0137 - val_loss: 0.0071\n",
      "Epoch 7/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 486ms/step - loss: 0.0127 - val_loss: 0.0058\n",
      "Epoch 8/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 389ms/step - loss: 0.0119 - val_loss: 0.0080\n",
      "Epoch 9/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 387ms/step - loss: 0.0115 - val_loss: 0.0047\n",
      "Epoch 10/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 536ms/step - loss: 0.0109 - val_loss: 0.0057\n",
      "Epoch 11/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 396ms/step - loss: 0.0101 - val_loss: 0.0066\n",
      "Epoch 12/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 332ms/step - loss: 0.0101 - val_loss: 0.0112\n",
      "Epoch 13/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 442ms/step - loss: 0.0098 - val_loss: 0.0049\n",
      "Epoch 14/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 452ms/step - loss: 0.0094 - val_loss: 0.0077\n",
      "Epoch 15/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 412ms/step - loss: 0.0093 - val_loss: 0.0036\n",
      "Epoch 16/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 439ms/step - loss: 0.0091 - val_loss: 0.0062\n",
      "Epoch 17/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 475ms/step - loss: 0.0089 - val_loss: 0.0047\n",
      "Epoch 18/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 331ms/step - loss: 0.0084 - val_loss: 0.0045\n",
      "Epoch 19/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 316ms/step - loss: 0.0083 - val_loss: 0.0064\n",
      "Epoch 20/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 474ms/step - loss: 0.0085 - val_loss: 0.0039\n",
      "Epoch 21/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 437ms/step - loss: 0.0082 - val_loss: 0.0030\n",
      "Epoch 22/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 386ms/step - loss: 0.0083 - val_loss: 0.0065\n",
      "Epoch 23/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 471ms/step - loss: 0.0085 - val_loss: 0.0028\n",
      "Epoch 24/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 467ms/step - loss: 0.0081 - val_loss: 0.0134\n",
      "Epoch 25/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 318ms/step - loss: 0.0085 - val_loss: 0.0051\n",
      "Epoch 26/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 367ms/step - loss: 0.0080 - val_loss: 0.0049\n",
      "Epoch 27/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 458ms/step - loss: 0.0078 - val_loss: 0.0035\n",
      "Epoch 28/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 403ms/step - loss: 0.0081 - val_loss: 0.0049\n",
      "Epoch 29/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 402ms/step - loss: 0.0077 - val_loss: 0.0034\n",
      "Epoch 30/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 501ms/step - loss: 0.0075 - val_loss: 0.0088\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step   \n",
      "Model saved at: D:\\Projects\\HPE-StoragePrediction\\notebooks\\models\\projects_monthly_forecast_model.keras\n",
      "Scaler saved at: D:\\Projects\\HPE-StoragePrediction\\notebooks\\scalers\\projects_monthly_scaler.pkl\n",
      "\n",
      "⚡ Processing /customer\n",
      "Epoch 1/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 351ms/step - loss: 0.2355 - val_loss: 0.1512\n",
      "Epoch 2/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 335ms/step - loss: 0.0719 - val_loss: 0.0283\n",
      "Epoch 3/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 416ms/step - loss: 0.0348 - val_loss: 0.0168\n",
      "Epoch 4/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 308ms/step - loss: 0.0277 - val_loss: 0.0166\n",
      "Epoch 5/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 320ms/step - loss: 0.0242 - val_loss: 0.0085\n",
      "Epoch 6/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 352ms/step - loss: 0.0226 - val_loss: 0.0266\n",
      "Epoch 7/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 411ms/step - loss: 0.0232 - val_loss: 0.0150\n",
      "Epoch 8/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 232ms/step - loss: 0.0219 - val_loss: 0.0205\n",
      "Epoch 9/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 237ms/step - loss: 0.0202 - val_loss: 0.0113\n",
      "Epoch 10/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 302ms/step - loss: 0.0195 - val_loss: 0.0066\n",
      "Epoch 11/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 367ms/step - loss: 0.0197 - val_loss: 0.0082\n",
      "Epoch 12/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 367ms/step - loss: 0.0192 - val_loss: 0.0136\n",
      "Epoch 13/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 307ms/step - loss: 0.0193 - val_loss: 0.0077\n",
      "Epoch 14/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 299ms/step - loss: 0.0191 - val_loss: 0.0026\n",
      "Epoch 15/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 334ms/step - loss: 0.0193 - val_loss: 0.0051\n",
      "Epoch 16/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 399ms/step - loss: 0.0182 - val_loss: 0.0049\n",
      "Epoch 17/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 289ms/step - loss: 0.0194 - val_loss: 0.0018\n",
      "Epoch 18/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 284ms/step - loss: 0.0198 - val_loss: 0.0027\n",
      "Epoch 19/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 301ms/step - loss: 0.0201 - val_loss: 0.0109\n",
      "Epoch 20/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 421ms/step - loss: 0.0178 - val_loss: 0.0029\n",
      "Epoch 21/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 309ms/step - loss: 0.0182 - val_loss: 0.0165\n",
      "Epoch 22/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 296ms/step - loss: 0.0178 - val_loss: 0.0069\n",
      "Epoch 23/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 293ms/step - loss: 0.0178 - val_loss: 0.0065\n",
      "Epoch 24/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 410ms/step - loss: 0.0177 - val_loss: 0.0123\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 458ms/step\n",
      "Model saved at: D:\\Projects\\HPE-StoragePrediction\\notebooks\\models\\customer_monthly_forecast_model.keras\n",
      "Scaler saved at: D:\\Projects\\HPE-StoragePrediction\\notebooks\\scalers\\customer_monthly_scaler.pkl\n",
      "\n",
      "⚡ Processing /info\n",
      "Epoch 1/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 434ms/step - loss: 0.3028 - val_loss: 0.1508\n",
      "Epoch 2/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 372ms/step - loss: 0.0856 - val_loss: 0.0088\n",
      "Epoch 3/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 503ms/step - loss: 0.0333 - val_loss: 0.0027\n",
      "Epoch 4/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 374ms/step - loss: 0.0240 - val_loss: 0.0022\n",
      "Epoch 5/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 374ms/step - loss: 0.0200 - val_loss: 0.0021\n",
      "Epoch 6/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 458ms/step - loss: 0.0180 - val_loss: 0.0031\n",
      "Epoch 7/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 434ms/step - loss: 0.0171 - val_loss: 0.0030\n",
      "Epoch 8/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 383ms/step - loss: 0.0162 - val_loss: 0.0019\n",
      "Epoch 9/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 417ms/step - loss: 0.0155 - val_loss: 0.0025\n",
      "Epoch 10/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 480ms/step - loss: 0.0143 - val_loss: 0.0021\n",
      "Epoch 11/50\n",
      "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 400ms/step - loss: 0.0137"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_dict = load_and_preprocess_data()\n",
    "    models, metrics = train_and_evaluate(data_dict)\n",
    "\n",
    "    for directory in data_dict:\n",
    "        if directory in metrics:\n",
    "            print(f\"\\n📊 {directory.upper()} PERFORMANCE\")\n",
    "            for horizon in HORIZONS:\n",
    "                if horizon in metrics[directory]:\n",
    "                    rmse = metrics[directory][horizon]['rmse']\n",
    "                    print(f\"{horizon.replace('_', ' ').title():<12} RMSE: {rmse:.2f} GB\")\n",
    "            plot_results(directory, data_dict[directory], metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
