{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IrIj1b3BMWxm",
    "outputId": "efe150a2-ed34-432f-f7e5-acf9e35dc4f5"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting pymongo\n",
      "  Downloading pymongo-4.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
      "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
      "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Downloading pymongo-4.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.4/1.4 MB\u001B[0m \u001B[31m27.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m313.6/313.6 kB\u001B[0m \u001B[31m18.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: dnspython, pymongo\n",
      "Successfully installed dnspython-2.7.0 pymongo-4.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "621OAEcU3QY6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pymongo\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "c4STbPMO3TcJ"
   },
   "outputs": [],
   "source": [
    "# MongoDB setup\n",
    "client = pymongo.MongoClient(\"mongodb+srv://bhavyanayak830:hpecppguys@cluster0.k0b3rqz.mongodb.net/\")\n",
    "db = client[\"storage_simulation\"]\n",
    "collection = db[\"usage_logs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "nOQxMu-g3AbO"
   },
   "outputs": [],
   "source": [
    "def load_data(directory=\"/scratch\"):\n",
    "    \"\"\"Load storage data for a specific directory from MongoDB\"\"\"\n",
    "    cursor = collection.find({\"directory\": directory})\n",
    "    df = pd.DataFrame(list(cursor))\n",
    "\n",
    "    # Basic preprocessing\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df = df.sort_values('timestamp')\n",
    "    df = df.drop_duplicates(subset=[\"timestamp\", \"directory\"])\n",
    "    if '_id' in df.columns:\n",
    "        df = df.drop(columns=['_id'])\n",
    "\n",
    "    # Add time-based features\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    df['day'] = df['timestamp'].dt.day\n",
    "    df['dayofweek'] = df['timestamp'].dt.dayofweek\n",
    "    df['month'] = df['timestamp'].dt.month\n",
    "\n",
    "    # Add rolling statistics that help capture drift and volatility\n",
    "    df['rolling_mean_24h'] = df['storage_gb'].rolling(window=96).mean()  # 24 hours = 96 15-min intervals\n",
    "    df['rolling_std_24h'] = df['storage_gb'].rolling(window=96).std()\n",
    "\n",
    "    # Add features to detect spikes and drops\n",
    "    df['storage_diff'] = df['storage_gb'].diff()\n",
    "    df['is_spike'] = (df['storage_diff'] > 10).astype(int)  # Detect large positive changes\n",
    "    df['is_drop'] = (df['storage_diff'] < -5).astype(int)   # Detect large negative changes\n",
    "\n",
    "    # Fill NaN values from rolling calculations and diff\n",
    "    df = df.fillna(method='bfill')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def aggregate_to_daily(df):\n",
    "    \"\"\"Aggregate 15-minute data to daily data with enhanced features\"\"\"\n",
    "    # Set timestamp as index and resample to daily frequency\n",
    "    df_daily = df.set_index('timestamp').resample('D').agg({\n",
    "        'storage_gb': 'last',          # Last storage value of the day\n",
    "        'added_gb': 'sum',             # Total added during the day\n",
    "        'deleted_gb': 'sum',           # Total deleted during the day\n",
    "        'updated_gb': 'sum',           # Total updated during the day\n",
    "        'rolling_mean_24h': 'last',    # Last daily rolling mean\n",
    "        'rolling_std_24h': 'last',     # Last daily rolling std\n",
    "        'is_spike': 'sum',             # Count of spikes in the day\n",
    "        'is_drop': 'sum'               # Count of drops in the day\n",
    "    }).reset_index()\n",
    "\n",
    "    # Add time-based features for daily data\n",
    "    df_daily['day'] = df_daily['timestamp'].dt.day\n",
    "    df_daily['dayofweek'] = df_daily['timestamp'].dt.dayofweek\n",
    "    df_daily['month'] = df_daily['timestamp'].dt.month\n",
    "\n",
    "    # Add lag features\n",
    "    for lag in [1, 2, 3, 7]:  # Include 1, 2, 3 and 7 day lags\n",
    "        df_daily[f'storage_lag_{lag}'] = df_daily['storage_gb'].shift(lag)\n",
    "\n",
    "    # Calculate 7-day rolling statistics\n",
    "    df_daily['rolling_mean_7d'] = df_daily['storage_gb'].rolling(window=7).mean()\n",
    "    df_daily['rolling_std_7d'] = df_daily['storage_gb'].rolling(window=7).std()\n",
    "\n",
    "    # Fill NaN values\n",
    "    df_daily = df_daily.fillna(method='bfill')\n",
    "\n",
    "    return df_daily\n",
    "\n",
    "def scale_data(data):\n",
    "    \"\"\"Scale the data for model training\"\"\"\n",
    "    # Extract target (storage_gb) to scale separately\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        y_data = data['storage_gb'].values.reshape(-1, 1)\n",
    "\n",
    "        # Remove target from features\n",
    "        X_data = data.drop('storage_gb', axis=1)\n",
    "\n",
    "        # Scale target\n",
    "        y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        y_scaled = y_scaler.fit_transform(y_data)\n",
    "\n",
    "        # Scale features\n",
    "        X_scaler = StandardScaler()\n",
    "        X_scaled = X_scaler.fit_transform(X_data)\n",
    "\n",
    "        return X_scaled, y_scaled, X_scaler, y_scaler, X_data.columns\n",
    "    else:\n",
    "        # Simple 1D array scaling\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled_data = scaler.fit_transform(data.reshape(-1, 1))\n",
    "        return scaled_data, scaler"
   ],
   "metadata": {
    "id": "MjDNaH99IFBP"
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def create_sequences(X, y, seq_length):\n",
    "    \"\"\"Create sequences for LSTM input\"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - seq_length):\n",
    "        X_seq.append(X[i:i+seq_length])\n",
    "        y_seq.append(y[i+seq_length])\n",
    "    return np.array(X_seq), np.array(y_seq)"
   ],
   "metadata": {
    "id": "midA3UwVIHN7"
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def create_multistep_sequences(X, y, input_seq_length, output_seq_length):\n",
    "    \"\"\"Create sequences for multi-step LSTM prediction\"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - input_seq_length - output_seq_length + 1):\n",
    "        X_seq.append(X[i:i+input_seq_length])\n",
    "        y_seq.append(y[i+input_seq_length:i+input_seq_length+output_seq_length])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "def build_enhanced_lstm_model(input_shape, output_shape=1):\n",
    "    \"\"\"Build and compile an enhanced LSTM model\"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    # First Bidirectional LSTM layer\n",
    "    model.add(Bidirectional(LSTM(128, activation='tanh', return_sequences=True),\n",
    "                         input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    # Second LSTM layer\n",
    "    model.add(LSTM(64, activation='tanh', return_sequences=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    # Dense layers\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(output_shape))\n",
    "\n",
    "    # Compile with Adam optimizer with custom learning rate\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "id": "7wgvDyXjIJfd"
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate_model(y_true, y_pred, scaler=None):\n",
    "    \"\"\"Calculate performance metrics\"\"\"\n",
    "    if scaler:\n",
    "        # Reshape data if needed\n",
    "        y_true_reshaped = y_true.reshape(-1, 1) if len(y_true.shape) == 1 else y_true\n",
    "        y_pred_reshaped = y_pred.reshape(-1, 1) if len(y_pred.shape) == 1 else y_pred\n",
    "\n",
    "        # Inverse transform if scaler provided\n",
    "        y_true = scaler.inverse_transform(y_true_reshaped)\n",
    "        y_pred = scaler.inverse_transform(y_pred_reshaped)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    return {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2\n",
    "    }"
   ],
   "metadata": {
    "id": "fvrFRLb7IMQu"
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def plot_predictions(actual, predicted, title):\n",
    "    \"\"\"Plot actual vs predicted values\"\"\"\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(actual, label='Actual', color='blue', linewidth=2)\n",
    "    plt.plot(predicted, label='Predicted', color='red', linestyle='--', linewidth=2)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('Time Steps', fontsize=12)\n",
    "    plt.ylabel('Storage (GB)', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "id": "5CLomp2LIPS_"
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train_daily_multistep_model(df_daily, forecast_horizon, seq_length=14):\n",
    "    \"\"\"Train an optimized LSTM model for multi-step daily predictions\"\"\"\n",
    "    # Prepare features and target\n",
    "    target_col = 'storage_gb'\n",
    "    feature_cols = [col for col in df_daily.columns if col != 'timestamp']\n",
    "\n",
    "    # Create dataframe with features\n",
    "    data = df_daily[feature_cols].copy()\n",
    "\n",
    "    # Scale data\n",
    "    X_scaled, y_scaled, X_scaler, y_scaler, feature_names = scale_data(data)\n",
    "\n",
    "    # Split into train, validation and test sets (60%, 20%, 20%)\n",
    "    train_size = int(len(X_scaled) * 0.6)\n",
    "    val_size = int(len(X_scaled) * 0.2)\n",
    "\n",
    "    X_train = X_scaled[:train_size]\n",
    "    X_val = X_scaled[train_size:train_size+val_size]\n",
    "    X_test = X_scaled[train_size+val_size:]\n",
    "\n",
    "    y_train = y_scaled[:train_size]\n",
    "    y_val = y_scaled[train_size:train_size+val_size]\n",
    "    y_test = y_scaled[train_size+val_size:]\n",
    "\n",
    "    # Create sequences for direct multi-step prediction\n",
    "    X_train_seq, y_train_seq = create_multistep_sequences(X_train, y_train, seq_length, forecast_horizon)\n",
    "    X_val_seq, y_val_seq = create_multistep_sequences(X_val, y_val, seq_length, forecast_horizon)\n",
    "    X_test_seq, y_test_seq = create_multistep_sequences(X_test, y_test, seq_length, forecast_horizon)\n",
    "\n",
    "    # Get the number of features\n",
    "    n_features = X_train.shape[1]\n",
    "\n",
    "    # Build model\n",
    "    model = build_enhanced_lstm_model((seq_length, n_features), forecast_horizon)\n",
    "\n",
    "    # Callbacks\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=20,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=10,\n",
    "        min_lr=0.0001,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_seq,\n",
    "        epochs=200,\n",
    "        batch_size=16,\n",
    "        validation_data=(X_val_seq, y_val_seq),\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Make predictions\n",
    "    train_predictions = model.predict(X_train_seq)\n",
    "    val_predictions = model.predict(X_val_seq)\n",
    "    test_predictions = model.predict(X_test_seq)\n",
    "\n",
    "    # Reshape for evaluation\n",
    "    y_train_reshaped = y_train_seq.reshape(y_train_seq.shape[0], forecast_horizon)\n",
    "    y_val_reshaped = y_val_seq.reshape(y_val_seq.shape[0], forecast_horizon)\n",
    "    y_test_reshaped = y_test_seq.reshape(y_test_seq.shape[0], forecast_horizon)\n",
    "\n",
    "    # Evaluate model\n",
    "    horizon_desc = f\"{forecast_horizon} days\"\n",
    "    print(f\"\\n--- Enhanced Daily Model Performance ({horizon_desc} horizon) ---\")\n",
    "\n",
    "    # Training metrics\n",
    "    train_metrics = evaluate_model(y_train_reshaped, train_predictions, y_scaler)\n",
    "    print(\"Training Metrics:\")\n",
    "    for metric, value in train_metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "    # Validation metrics\n",
    "    val_metrics = evaluate_model(y_val_reshaped, val_predictions, y_scaler)\n",
    "    print(\"\\nValidation Metrics:\")\n",
    "    for metric, value in val_metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "    # Test metrics\n",
    "    test_metrics = evaluate_model(y_test_reshaped, test_predictions, y_scaler)\n",
    "    print(\"\\nTest Metrics:\")\n",
    "    for metric, value in test_metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "    # Plot learning curves\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(history.history['loss'], label='Training Loss', color='blue')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss', color='red')\n",
    "    plt.title(f'Training vs Validation Loss ({horizon_desc} horizon)', fontsize=16)\n",
    "    plt.xlabel('Epochs', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot predictions for test set\n",
    "    # For visualization, we'll plot the first 5 test sequences\n",
    "    for i in range(min(5, len(test_predictions))):\n",
    "        # Get actual and predicted values\n",
    "        y_actual = y_scaler.inverse_transform(y_test_seq[i].reshape(-1, 1)).flatten()\n",
    "        y_pred = y_scaler.inverse_transform(test_predictions[i].reshape(-1, 1)).flatten()\n",
    "\n",
    "        # Plot\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        plt.plot(range(forecast_horizon), y_actual, 'b-', label='Actual', linewidth=2)\n",
    "        plt.plot(range(forecast_horizon), y_pred, 'r--', label='Predicted', linewidth=2)\n",
    "        plt.title(f'Test Sample {i+1}: {forecast_horizon}-Day Forecast', fontsize=16)\n",
    "        plt.xlabel('Days Ahead', fontsize=12)\n",
    "        plt.ylabel('Storage (GB)', fontsize=12)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.legend(fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return model, history, y_scaler, test_metrics"
   ],
   "metadata": {
    "id": "O0_v792aIRcN"
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    print(\"Loading data for /scratch directory...\")\n",
    "    df = load_data(directory=\"/scratch\")\n",
    "    print(f\"Loaded {len(df)} 15-minute records with enhanced features.\")\n",
    "\n",
    "    # Aggregate to daily for longer horizon predictions\n",
    "    print(\"\\nAggregating data to daily frequency with enhanced features...\")\n",
    "    df_daily = aggregate_to_daily(df)\n",
    "    print(f\"Created {len(df_daily)} daily records.\")\n",
    "\n",
    "    # Train and evaluate enhanced models for different forecast horizons\n",
    "    print(\"\\n========== Enhanced Daily Forecasting Models ==========\")\n",
    "\n",
    "    # 7-day forecast model\n",
    "    print(\"\\n----- Training 7-Day Forecast Model -----\")\n",
    "    model_7day, history_7day, scaler_7day, metrics_7day = train_daily_multistep_model(\n",
    "        df_daily, forecast_horizon=7, seq_length=14\n",
    "    )\n",
    "\n",
    "    # 30-day forecast model\n",
    "    print(\"\\n----- Training 30-Day Forecast Model -----\")\n",
    "    model_30day, history_30day, scaler_30day, metrics_30day = train_daily_multistep_model(\n",
    "        df_daily, forecast_horizon=30, seq_length=21\n",
    "    )\n",
    "\n",
    "    # 90-day forecast model\n",
    "    print(\"\\n----- Training 90-Day Forecast Model -----\")\n",
    "    model_90day, history_90day, scaler_90day, metrics_90day = train_daily_multistep_model(\n",
    "        df_daily, forecast_horizon=90, seq_length=30\n",
    "    )\n",
    "\n",
    "    print(\"\\n========== All enhanced models trained and evaluated successfully ==========\")\n",
    "\n",
    "    # Summary of results\n",
    "    print(\"\\n========== Performance Summary ==========\")\n",
    "    print(f\"7-Day Forecast R2: {metrics_7day['R2']:.4f}\")\n",
    "    print(f\"30-Day Forecast R2: {metrics_30day['R2']:.4f}\")\n",
    "    print(f\"90-Day Forecast R2: {metrics_90day['R2']:.4f}\")"
   ],
   "metadata": {
    "id": "pl-asi8AITgw"
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "metadata": {
    "id": "ps2Gk4rJIZa2"
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "id": "SK4pUN3_Ihr_"
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "id": "nqBMf-NjIlLS"
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "id": "hSi4C3iAIqEq"
   },
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R2TphJUEIu9r",
    "outputId": "48c370be-f559-4f9a-92f3-36517c960710"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading data for /scratch directory...\n",
      "Loaded 71618 15-minute records.\n",
      "\n",
      "========== 15-Minute Forecasting Model ==========\n",
      "Epoch 1/100\n",
      "\u001B[1m 167/1431\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m2:22\u001B[0m 113ms/step - loss: 0.0379"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
