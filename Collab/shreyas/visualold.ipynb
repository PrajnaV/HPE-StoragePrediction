{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storage Metrics Data Analysis\n",
    "\n",
    "This notebook analyzes the storage metrics data generated from October 1, 2024, to March 14, 2025. We'll explore patterns, trends, and insights in the data through various visualizations and analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set plot styling\n",
    "plt.style.use('ggplot')\n",
    "sns.set(font_scale=1.2)\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "\n",
    "# Load the data\n",
    "file_path = 'storage_metrics_history.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert timestamp to datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# Create date and hour columns for time-based analysis\n",
    "df['date'] = df['timestamp'].dt.date\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['day_of_week'] = df['timestamp'].dt.day_name()\n",
    "df['month'] = df['timestamp'].dt.month_name()\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Data spans from {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "print(f\"Total records: {len(df)}\")\n",
    "print(f\"Directories: {', '.join(df['directory'].unique())}\")\n",
    "\n",
    "# Show first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Data summary\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Storage Growth Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a daily snapshot of space usage per directory\n",
    "daily_space = df.groupby(['date', 'directory'])['current_space_gb'].last().unstack()\n",
    "\n",
    "# Plot storage growth over time\n",
    "plt.figure(figsize=(14, 7))\n",
    "daily_space.plot()\n",
    "plt.title('Storage Usage Growth Over Time', fontsize=16)\n",
    "plt.ylabel('Storage Used (GB)', fontsize=14)\n",
    "plt.xlabel('Date', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(title='Directory', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate month-over-month growth rates\n",
    "# Group by month and directory to get the last value of each month\n",
    "df['year_month'] = df['timestamp'].dt.to_period('M')\n",
    "monthly_space = df.groupby(['year_month', 'directory'])['current_space_gb'].last().unstack()\n",
    "\n",
    "# Calculate growth rates\n",
    "monthly_growth = monthly_space.pct_change() * 100\n",
    "\n",
    "# Display growth rates\n",
    "plt.figure(figsize=(14, 7))\n",
    "monthly_growth.plot(kind='bar')\n",
    "plt.title('Month-over-Month Storage Growth Rate (%)', fontsize=16)\n",
    "plt.ylabel('Growth Rate (%)', fontsize=14)\n",
    "plt.xlabel('Month', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(title='Directory', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. File Operation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze file operations by directory\n",
    "# Aggregate daily operations\n",
    "daily_ops = df.groupby(['date', 'directory']).agg({\n",
    "    'files_added': 'sum',\n",
    "    'files_deleted': 'sum',\n",
    "    'files_modified': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Calculate total operations\n",
    "daily_ops['total_operations'] = daily_ops['files_added'] + daily_ops['files_deleted'] + daily_ops['files_modified']\n",
    "\n",
    "# Calculate 7-day rolling average\n",
    "rolling_ops = daily_ops.set_index('date').groupby('directory')[['total_operations']].rolling(7).mean().reset_index()\n",
    "\n",
    "# Plot rolling average operations by directory\n",
    "plt.figure(figsize=(14, 7))\n",
    "for directory in daily_ops['directory'].unique():\n",
    "    data = rolling_ops[rolling_ops['directory'] == directory]\n",
    "    plt.plot(data['date'], data['total_operations'], label=directory)\n",
    "\n",
    "plt.title('7-Day Rolling Average of Daily File Operations', fontsize=16)\n",
    "plt.ylabel('Operations Count (7-day avg)', fontsize=14)\n",
    "plt.xlabel('Date', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(title='Directory', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a stacked bar chart of operation types by directory\n",
    "ops_by_dir = df.groupby('directory').agg({\n",
    "    'files_added': 'sum',\n",
    "    'files_deleted': 'sum',\n",
    "    'files_modified': 'sum'\n",
    "})\n",
    "\n",
    "# Plot\n",
    "ops_by_dir.plot(kind='bar', stacked=True, figsize=(12, 6))\n",
    "plt.title('Total File Operations by Directory', fontsize=16)\n",
    "plt.ylabel('Number of Operations', fontsize=14)\n",
    "plt.xlabel('Directory', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(title='Operation Type', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Temporal Patterns Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze operations by hour of day\n",
    "hourly_ops = df.groupby(['hour', 'directory']).agg({\n",
    "    'files_added': 'sum',\n",
    "    'files_deleted': 'sum',\n",
    "    'files_modified': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "hourly_ops['total_operations'] = hourly_ops['files_added'] + hourly_ops['files_deleted'] + hourly_ops['files_modified']\n",
    "\n",
    "# Plot operations by hour for each directory\n",
    "plt.figure(figsize=(14, 8))\n",
    "for directory in hourly_ops['directory'].unique():\n",
    "    data = hourly_ops[hourly_ops['directory'] == directory]\n",
    "    plt.plot(data['hour'], data['total_operations'], marker='o', label=directory)\n",
    "\n",
    "plt.title('File Operations by Hour of Day', fontsize=16)\n",
    "plt.ylabel('Total Operations', fontsize=14)\n",
    "plt.xlabel('Hour of Day', fontsize=14)\n",
    "plt.xticks(range(0, 24))\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(title='Directory', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze operations by day of week\n",
    "# Create ordered day of week\n",
    "days_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "df['day_of_week'] = pd.Categorical(df['day_of_week'], categories=days_order, ordered=True)\n",
    "\n",
    "day_ops = df.groupby(['day_of_week', 'directory']).agg({\n",
    "    'files_added': 'sum',\n",
    "    'files_deleted': 'sum',\n",
    "    'files_modified': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "day_ops['total_operations'] = day_ops['files_added'] + day_ops['files_deleted'] + day_ops['files_modified']\n",
    "\n",
    "# Plot heatmap of operations by day of week and directory\n",
    "day_ops_pivot = day_ops.pivot(index='day_of_week', columns='directory', values='total_operations')\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(day_ops_pivot, annot=True, fmt=\",.0f\", cmap=\"YlGnBu\")\n",
    "plt.title('File Operations by Day of Week and Directory', fontsize=16)\n",
    "plt.ylabel('Day of Week', fontsize=14)\n",
    "plt.xlabel('Directory', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Storage Efficiency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate the ratio of files to storage space\n",
    "latest_stats = df.groupby('directory').last().reset_index()\n",
    "latest_stats['GB_per_file'] = latest_stats['current_space_gb'] / latest_stats['total_files']\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='directory', y='GB_per_file', data=latest_stats)\n",
    "plt.title('Average File Size by Directory (GB/file)', fontsize=16)\n",
    "plt.ylabel('Average Size (GB/file)', fontsize=14)\n",
    "plt.xlabel('Directory', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate net change in storage (added - deleted) over time\n",
    "df['net_size_change_gb'] = df['size_added_gb'] - df['size_deleted_gb']\n",
    "df['cumulative_change'] = df.groupby('directory')['net_size_change_gb'].cumsum()\n",
    "\n",
    "# Plot cumulative net change by directory\n",
    "plt.figure(figsize=(14, 7))\n",
    "for directory in df['directory'].unique():\n",
    "    data = df[df['directory'] == directory].copy()\n",
    "    # Resample to daily for smoother plot\n",
    "    daily_data = data.set_index('timestamp').resample('D')[['cumulative_change']].last()\n",
    "    plt.plot(daily_data.index, daily_data['cumulative_change'], label=directory)\n",
    "\n",
    "plt.title('Cumulative Net Storage Change Over Time', fontsize=16)\n",
    "plt.ylabel('Cumulative Net Change (GB)', fontsize=14)\n",
    "plt.xlabel('Date', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(title='Directory', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Directory Comparison and Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate daily total operations and identify outliers\n",
    "daily_total = df.groupby(['date', 'directory']).agg({\n",
    "    'files_added': 'sum',\n",
    "    'files_deleted': 'sum',\n",
    "    'files_modified': 'sum',\n",
    "    'size_added_gb': 'sum',\n",
    "    'size_deleted_gb': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "daily_total['total_files_ops'] = daily_total['files_added'] + daily_total['files_deleted'] + daily_total['files_modified']\n",
    "daily_total['total_size_ops'] = daily_total['size_added_gb'] + daily_total['size_deleted_gb']\n",
    "\n",
    "# Identify outlier days (days with operations > 2 standard deviations from mean)\n",
    "outliers = {}\n",
    "for directory in daily_total['directory'].unique():\n",
    "    dir_data = daily_total[daily_total['directory'] == directory]\n",
    "    mean = dir_data['total_files_ops'].mean()\n",
    "    std = dir_data['total_files_ops'].std()\n",
    "    threshold = mean + 2 * std\n",
    "    \n",
    "    outlier_days = dir_data[dir_data['total_files_ops'] > threshold]\n",
    "    if not outlier_days.empty:\n",
    "        outliers[directory] = outlier_days[['date', 'total_files_ops']]\n",
    "\n",
    "# Display outliers\n",
    "for directory, outlier_data in outliers.items():\n",
    "    print(f\"Outlier days for {directory}:\")\n",
    "    print(outlier_data)\n",
    "    print(\"---\")\n",
    "\n",
    "# Plot daily operations with outliers highlighted\n",
    "plt.figure(figsize=(14, 8))\n",
    "for directory in daily_total['directory'].unique():\n",
    "    dir_data = daily_total[daily_total['directory'] == directory]\n",
    "    plt.plot(dir_data['date'], dir_data['total_files_ops'], alpha=0.7, label=directory)\n",
    "    \n",
    "    # Highlight outliers if any\n",
    "    if directory in outliers:\n",
    "        outlier_data = outliers[directory]\n",
    "        for _, row in outlier_data.iterrows():\n",
    "            plt.scatter(row['date'], row['total_files_ops'], color='red', s=100, zorder=5)\n",
    "\n",
    "plt.title('Daily File Operations with Outliers Highlighted (Red)', fontsize=16)\n",
    "plt.ylabel('Total File Operations', fontsize=14)\n",
    "plt.xlabel('Date', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(title='Directory', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze correlation between operations\n",
    "# Group by day and directory\n",
    "corr_data = daily_total[['directory', 'files_added', 'files_deleted', 'files_modified', \n",
    "                         'size_added_gb', 'size_deleted_gb']]\n",
    "\n",
    "# Calculate correlation matrices for each directory\n",
    "for directory in corr_data['directory'].unique():\n",
    "    dir_data = corr_data[corr_data['directory'] == directory].drop('directory', axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    corr_matrix = dir_data.corr()\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    plt.title(f'Correlation Matrix for {directory}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Forecasting and Future Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Use a simple linear extrapolation to forecast future storage needs\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Convert date to numeric for regression\n",
    "daily_space = daily_space.reset_index()\n",
    "daily_space['date_num'] = (pd.to_datetime(daily_space['date']) - pd.to_datetime('2024-10-01')).dt.days\n",
    "\n",
    "# Forecast next 30 days for each directory\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Get the last date in the data\n",
    "last_date = pd.to_datetime(daily_space['date'].max())\n",
    "future_dates = [last_date + timedelta(days=i) for i in range(1, 31)]\n",
    "future_date_nums = [(date - pd.to_datetime('2024-10-01')).days for date in future_dates]\n",
    "\n",
    "for directory in df['directory'].unique():\n",
    "    # Get data for this directory\n",
    "    dir_data = daily_space[['date', 'date_num', directory]].dropna()\n",
    "    \n",
    "    # Fit linear regression model\n",
    "    model = LinearRegression()\n",
    "    X = dir_data['date_num'].values.reshape(-1, 1)\n",
    "    y = dir_data[directory].values\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Plot actual data\n",
    "    plt.plot(pd.to_datetime(dir_data['date']), dir_data[directory], label=f'{directory} (Actual)')\n",
    "    \n",
    "    # Predict future values\n",
    "    future_X = np.array(future_date_nums).reshape(-1, 1)\n",
    "    future_y = model.predict(future_X)\n",
    "    \n",
    "    # Plot predictions\n",
    "    plt.plot(future_dates, future_y, '--', label=f'{directory} (Forecast)')\n",
    "\n",
    "plt.title('Storage Growth Forecast (Next 30 Days)', fontsize=16)\n",
    "plt.ylabel('Storage Used (GB)', fontsize=14)\n",
    "plt.xlabel('Date', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate when directories might reach capacity\n",
    "print(\"Projected days until directories reach 1TB (1,000 GB):\")\n",
    "for directory in df['directory'].unique():\n",
    "    dir_data = daily_space[['date_num', directory]].dropna()\n",
    "    model = LinearRegression()\n",
    "    X = dir_data['date_num'].values.reshape(-1, 1)\n",
    "    y = dir_data[directory].values\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Current size\n",
    "    current_size = dir_data[directory].iloc[-1]\n",
    "    \n",
    "    # Calculate days until 1TB\n",
    "    if model.coef_[0] > 0:  # Only if growth is positive\n",
    "        days_to_1tb = (1000 - current_size) / model.coef_[0]\n",
    "        target_date = last_date + timedelta(days=int(days_to_1tb))\n",
    "        print(f\"{directory}: {int(days_to_1tb)} days (around {target_date.strftime('%Y-%m-%d')})\")\n",
    "    else:\n",
    "        print(f\"{directory}: No growth or negative growth trend detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Insights and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the analysis above, here are the key insights and recommendations:\n",
    "\n",
    "### Storage Growth Patterns\n",
    "- The directories show different growth rates, with [directory] growing fastest\n",
    "- We can expect [directory] to reach 1TB capacity first, around [date]\n",
    "\n",
    "### Usage Patterns\n",
    "- Peak activity occurs during [time/day], suggesting this is when most users are active\n",
    "- [Directory] shows the highest activity level and might need additional resources\n",
    "- Weekend usage is significantly [higher/lower] than weekday usage\n",
    "\n",
    "### File Operations\n",
    "- [Directory] has the highest file creation rate\n",
    "- [Directory] has the highest file modification rate\n",
    "- [Directory] has the highest deletion rate\n",
    "\n",
    "### Recommendations\n",
    "1. Consider increasing storage capacity for [directory] by [date]\n",
    "2. Implement automated cleanup policies for [directory] to manage growth\n",
    "3. Schedule maintenance and backups during low-activity periods ([time/day])\n",
    "4. Monitor [directory] for unusual activity patterns based on the identified outliers\n",
    "5. Consider implementing tiered storage solutions for [directory] to optimize costs\n",
    "\n",
    "### Further Analysis\n",
    "- Deeper investigation into file types stored in each directory\n",
    "- User-specific analysis to identify heavy users\n",
    "- Cost optimization analysis for storage utilization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Exporting Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Export key metrics to CSV for reporting\n",
    "# Monthly summary by directory\n",
    "monthly_summary = df.groupby(['year_month', 'directory']).agg({\n",
    "    'files_added': 'sum',\n",
    "    'files_deleted': 'sum', \n",
    "    'files_modified': 'sum',\n",
    "    'size_added_gb': 'sum',\n",
    "    'size_deleted_gb': 'sum',\n",
    "    'size_modified_gb': 'sum',\n",
    "    'current_space_gb': 'last',\n",
    "    'total_files': 'last'\n",
    "}).reset_index()\n",
    "\n",
    "# Calculate net growth\n",
    "monthly_summary['net_size_growth_gb'] = monthly_summary['size_added_gb'] - monthly_summary['size_deleted_gb']\n",
    "monthly_summary['net_files_growth'] = monthly_summary['files_added'] - monthly_summary['files_deleted']\n",
    "\n",
    "# Export to CSV\n",
    "monthly_summary.to_csv('storage_monthly_summary.csv', index=False)\n",
    "print(\"Monthly summary exported to 'storage_monthly_summary.csv'\")\n",
    "\n",
    "# Display the summary table\n",
    "monthly_summary.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}